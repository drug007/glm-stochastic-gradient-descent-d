{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Models and Stochastic Gradient Descent in D\n",
    "\n",
    "The [Mir GLAS](https://github.com/libmir/mir-glas) library has shown that D language is capable of high performance calculations to rival those written in C and C++. Mathematical analysis libraries native to D are however still in short supply. In this blog we roll our own Gamma Generalized Linear Models (GLM) written in optimization style using the Newton-Raphson method and we carry out linear regression using Stochastic Gradient Descent (SGD).\n",
    "\n",
    "## Mathematical preliminaries\n",
    "\n",
    "The main aim of a regression algorithm is to find a set of coefficients ($\\beta$) that maximize the likelihood of a target variable $(y)$ given a set of explanatory variables $(x)$. The algorithm makes assumptions regarding the distribution of the target variable, and the independence of observations of the explanatory variables.\n",
    "\n",
    "### Likelihood functions\n",
    "\n",
    "The likelihood function represents assumption of the distribution of the target variable. The likelhood function for the Gamma distribution is given by:\n",
    "\n",
    "$$\n",
    "L(x) = \\frac{1}{\\Gamma(x)\\theta^{k}} x^{k-1} e^{-\\frac{x}{\\theta}}\n",
    "$$\n",
    "\n",
    "The mean of the distribution is given by:\n",
    "\n",
    "$$\n",
    "\\mu = k \\theta\n",
    "$$\n",
    "\n",
    "The variance of the distribution is given by:\n",
    "\n",
    "$$\n",
    "\\sigma^2 = k \\theta^2\n",
    "$$\n",
    "\n",
    "The GLM dispersion in Gamma is given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\phi} = \\frac{X^2}{n - p}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of observations and $p$ is the number of parameters and $X^2$ is given by:\n",
    "\n",
    "$$\n",
    "X^2 = \\sum{\\frac{(y_i - \\mu_i)^2}{\\mu_i^2}}\n",
    "$$\n",
    "\n",
    "The parameter $k$ is given by:\n",
    "\n",
    "$$\n",
    "k = \\frac{1}{\\phi}\n",
    "$$\n",
    "\n",
    "and the Normal distribution likelihood function is given by:\n",
    "\n",
    "$$\n",
    "L(x) = \\frac{1}{2\\pi \\sigma^2}e^{-\\frac{(y - \\mu)^2}{2\\sigma}}\n",
    "$$\n",
    "\n",
    "### Gradients and Curvatures\n",
    "\n",
    "In practice however, we actually maximise the log-likelihood $(l)$ - that is the log of the likelihood function taken over the whole data set. The gradient function for the Gamma log-likelihood (we are assuming a log link $\\mu = e^{x\\beta}$) is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial \\beta_j} = -k x_j + k y x_j e^{-x\\beta}\n",
    "$$\n",
    "\n",
    "and its curvature is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 l}{\\partial \\beta_j \\beta_l} = -y x_j x_l e^{-x\\beta}\n",
    "$$\n",
    "\n",
    "The gradient function for the Normal log-likelihood is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial \\beta_j} = x_j \\frac{(y - x\\beta)}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "we shall only by doing gradient descent on linear regression but the curvature for the Normal log-likelihood is given below for completeness:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 l}{\\partial \\beta_j \\beta_l} = -\\frac{x_l x_j}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "where $\\sigma^2 = \\hat{\\phi}$\n",
    "\n",
    "## Regression as Optimization\n",
    "\n",
    "Regression is thus an optimization algorithm that maximizes the log-likelihood function for a set of $\\beta$ coefficients. It can thus be solved by numerical optimization.\n",
    "\n",
    "### Newton-Raphson Algorithm\n",
    "\n",
    "\n",
    "The Newton-Raphson method is an iterative optimization algorithm given by:\n",
    "\n",
    "$$\n",
    "\\beta_{n+1} = \\beta_n - H^{-1}g\n",
    "$$\n",
    "\n",
    "where $g$ is the gradient vector and $H$ is the curvature matrix. Stopping criterion is \n",
    "\n",
    "$$\n",
    "||\\beta_{new} - \\beta_{old}|| < \\epsilon_1 \\quad \\cap \\quad |l_{new} - l_{old}| < \\epsilon_2\n",
    "$$\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "The Gradient Descent algorithm is given by:\n",
    "\n",
    "$$\n",
    "\\beta_{n + 1} = \\beta_n + \\eta g\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM and SGD in D\n",
    "\n",
    "The scripts for DLM and SGD in D is given in the [D script](https://github.com/dataPulverizer/glm-stochastic-gradient-descent-d/blob/master/code/script.d) file.\n",
    "\n",
    "### Data I/O\n",
    "\n",
    "Below is the D function `read_yX` created for reading numeric data from a csv file and returning a `tuple` containing the `x` feature or design matrix and `y` the target variable vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><code>\n",
    "auto read_yX(char sep = ',')(string name)\n",
    "{\n",
    "\tauto file = File(name, \"r\");\n",
    "\t// Reads data into a double[][] array\n",
    "\tauto data = file.byLine(KeepTerminator.no)\n",
    "\t                .map!(a => a.split(sep)\n",
    "\t                \t.map!(b => to!double(b))\n",
    "\t                \t.array())\n",
    "\t                .array();\n",
    "\tauto y = data.map!(a => a[0]).array();\n",
    "\tauto x = data.map!(a => a[1 .. $]).array();\n",
    "\treturn tuple(y, x);\n",
    "}\n",
    "</code></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLAS and Lapacke functions\n",
    "\n",
    "Some funtions are imported from the `BLAS` and the `Lapacke` libraries. For more details of these algorithms see the [IBM ESSL documentation](https://www.ibm.com/support/knowledgecenter/en/SSFHY8/essl_content.html).\n",
    "\n",
    "<b>\n",
    "```\n",
    "extern(C) @nogc nothrow{\n",
    "void cblas_dgemm(in CBLAS_LAYOUT layout, in CBLAS_TRANSPOSE TransA,\n",
    "                 in CBLAS_TRANSPOSE TransB, in int M, in int N,\n",
    "                 in int K, in double alpha, in double  *A,\n",
    "                 in int lda, in double  *B, in int ldb,\n",
    "                 in double beta, double  *C, in int ldc);\n",
    "\n",
    "alias int lapack_int;\n",
    "alias int MKL_INT;\n",
    "/* See IBM ESSL documentation for more details */\n",
    "lapack_int LAPACKE_dgetrf(int matrix_layout, lapack_int m,\n",
    "\t        lapack_int n, double* a, lapack_int lda, \n",
    "\t        lapack_int* ipiv);\n",
    "lapack_int LAPACKE_dgetri(int matrix_layout, lapack_int n, \n",
    "\t        double* a, lapack_int lda, in lapack_int* ipiv);\n",
    "/* Function for calculating the norm of a vector */\n",
    "double cblas_dnrm2 (in MKL_INT n , in double* x , in MKL_INT incx);\n",
    "}\n",
    "```\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used in analysis and convenient interface functions are given below. The `norm2` $(||x||)$ interface function is given below:\n",
    "\n",
    "<b>\n",
    "```\n",
    "/* Norm2 function */\n",
    "double norm2(int incr = 1)(double[] x)\n",
    "{\n",
    "\treturn cblas_dnrm2(cast(int)x.length, x.ptr , incr);\n",
    "}\n",
    "```\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function for the in-place inverse of a matrix\n",
    "\n",
    "<b>```\n",
    "/* Function for in place inverse of a matrix */\n",
    "void inverse(int layout = LAPACK_ROW_MAJOR)(ref double[] matrix){\n",
    "\tint p = cast(int)sqrt(cast(double)matrix.length);\n",
    "\tint[] ipiv; ipiv.length = p;\n",
    "\tint info = LAPACKE_dgetrf(layout, p,\n",
    "\t            p, matrix.ptr, p, ipiv.ptr);\n",
    "\tassert(info == 0, \"Illegal value from function LAPACKE_dgetrf\");\n",
    "\tinfo = LAPACKE_dgetri(layout, p, \n",
    "\t            matrix.ptr, p, ipiv.ptr);\n",
    "\tassert(info == 0, \"Illegal value from function LAPACKE_dgetri\");\n",
    "}\n",
    "```</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function for matrix multiplication\n",
    "\n",
    "<b>\n",
    "```\n",
    "/* Matrix multiplication */\n",
    "double[] matmult(CBLAS_LAYOUT layout = CblasRowMajor, CBLAS_TRANSPOSE transA = CblasNoTrans, \n",
    "\t         CBLAS_TRANSPOSE transB = CblasNoTrans)\n",
    "             (double[] a, double[] b, int[2] dimA, int[2] dimB){\n",
    "\t         \n",
    "\t         double alpha = 1.;\n",
    "\t         double beta = 0.;\n",
    "\t         int m = transA == CblasNoTrans ? dimA[0] : dimA[1];\n",
    "\t         int n = transB == CblasNoTrans ? dimB[1] : dimB[0];\n",
    "\t         double[] c; //set this length\n",
    "\t         \n",
    "\t         int k = transA == CblasNoTrans ? dimA[1] : dimA[0];\n",
    "\t         \n",
    "\t         int lda, ldb, ldc;\n",
    "\t         if(transA == CblasNoTrans)\n",
    "\t         \tlda = layout == CblasRowMajor? k: m;\n",
    "\t         else\n",
    "\t         \tlda = layout == CblasRowMajor? m: k;\n",
    "\t         \n",
    "\t         if(transB == CblasNoTrans)\n",
    "\t         \tldb = layout == CblasRowMajor? n: k;\n",
    "\t         else\n",
    "\t         \tldb = layout == CblasRowMajor? k: n;\n",
    "\n",
    "\t         ldc = layout == CblasRowMajor ? n : m;\n",
    "\t         c.length = layout == CblasRowMajor ? ldc*m : ldc*n;\n",
    "\n",
    "\t         cblas_dgemm(layout, transA, transB, m, n,\n",
    "                 k, alpha, a.ptr, lda, b.ptr, ldb,\n",
    "                 beta, c.ptr, ldc);\n",
    "\t         return c;\n",
    "}\n",
    "```\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D code for Gamma regression\n",
    "\n",
    "The style of the code in the rest of this article happens to be written in a **functional style**. The functional style seems easier to write (if not read) these types of algorithms.\n",
    "\n",
    "<b>```\n",
    "/* Calculation of mu, k, xB, and n */\n",
    "mixin template gParamCalcs()\n",
    "{\n",
    "\tauto n = x.length, p = x[0].length;\n",
    "\tauto xB = zip(pars.repeat().take(n).array(), x)\n",
    "\t           .map!(a => zip(a[0], a[1])\n",
    "\t           \t            .map!(a => a[0]*a[1])\n",
    "\t           \t            .reduce!((a, b) => a + b))\n",
    "\t           .array();\n",
    "\tauto mu = xB.map!(a => exp(a)).array();\n",
    "\tauto k = (n - p)/zip(y, mu)\n",
    "\t                .map!(a => pow(a[0] - a[1], 2)/pow(a[1], 2))\n",
    "\t                .reduce!((a, b) => a + b);\n",
    "}\n",
    "```</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the implementation for the Gamma log-likelihood given previously omitting the Gamma function term which is not required\n",
    "\n",
    "<b>```\n",
    "/* This is a loglik for gamma distribution (omits gamma term) ... */\n",
    "T gLogLik(T)(T[] pars, T[][] x, T[] y)\n",
    "{\n",
    "\tmixin gParamCalcs;\n",
    "\tauto ll = zip(k.repeat().take(n).array(), xB, y, mu)\n",
    "\t          .map!(a => -a[0]*a[1] + a[0]*log(a[0]) + \n",
    "\t          \t          (a[0] - 1)*log(a[2]) - a[2]*a[0]/a[3])\n",
    "\t          .reduce!((a, b) => a + b);\n",
    "\treturn ll;\n",
    "}\n",
    "```</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions for the Gamma gradient and curvature are given by:\n",
    "\n",
    "<b>```\n",
    "T[] gGradient(T)(T[] pars, T[][] x, T[] y)\n",
    "{\n",
    "\tmixin gParamCalcs;\n",
    "\tauto output = zip(k.repeat().take(n).array(), x, y, mu)\n",
    "\t            .map!(a => \n",
    "\t            \tzip(a[0].repeat().take(a[1].length),\n",
    "\t            \t\ta[1],\n",
    "\t            \t\ta[2].repeat().take(a[1].length),\n",
    "\t            \t\ta[3].repeat().take(a[1].length))\n",
    "\t            \t.map!(a => -a[0]*a[1] + a[1]*a[0]*(a[2]/a[3]))\n",
    "\t            \t.array())\n",
    "\t            .reduce!((a, b) => zip(a, b).map!(a => a[0] + a[1]).array())\n",
    "\t            .array();\n",
    "\treturn output;\n",
    "}\n",
    "```\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>```\n",
    "auto gCurvature(T)(T[] pars, T[][] x, T[] y)\n",
    "{\n",
    "\tmixin gParamCalcs;\n",
    "\tauto xPrime = zip(k.repeat().take(n).array(), x, y, mu)\n",
    "\t                .map!( a => \n",
    "\t                \tzip(a[0].repeat().take(a[1].length),\n",
    "\t            \t\t    a[1],\n",
    "\t            \t\t    a[2].repeat().take(a[1].length),\n",
    "\t            \t\t    a[3].repeat().take(a[1].length))\n",
    "\t            \t\t.map!(a => -a[2]*(a[0]/a[3])*a[1])\n",
    "\t            \t\t.array())\n",
    "\t                .array();\n",
    "    int[2] dims = [cast(int)n, cast(int)p];\n",
    "\tT[] curv = matmult!(CblasRowMajor, CblasTrans, CblasNoTrans)\n",
    "             (xPrime.reduce!((a, b) => a ~ b).array(),\n",
    "             \tx.reduce!((a, b) => a ~ b).array(), \n",
    "             \tdims, dims);\n",
    "\treturn(curv);\n",
    "}\n",
    "```\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Raphson method are given by:\n",
    "\n",
    "<b>```\n",
    "T[] newtonRaphson(T, alias objFun = gLogLik, alias gradFun = gGradient, alias curveFun = gCurvature)\n",
    "                     (T[] pars, T[][] x, T[] y, T tol = 1e-5, T ltol = 1e-5)\n",
    "{\n",
    "\tT[] parsPrev = pars.dup;\n",
    "\tT ll = objFun(pars, x, y), llPrev = ll, llDiff = 1., parsDiff = 1.;\n",
    "\tint info, ipiv, n = cast(int)x.length, p = cast(int)x[0].length;\n",
    "\tT[] grad, curv, delta = T(0).repeat().take(p).array();\n",
    "\twhile((parsDiff > tol) | (llDiff > ltol))\n",
    "\t{\n",
    "\t\tcurv = curveFun(parsPrev, x, y);\n",
    "\t\tinverse(curv);\n",
    "\t\tgrad = gradFun(parsPrev, x, y);\n",
    "\t\t// Multiply the gradient by the hessian\n",
    "\t\tdelta = matmult!(CblasRowMajor, CblasNoTrans, CblasNoTrans)\n",
    "                        (curv, grad, [p, p], [p, 1]);\n",
    "\t\tpars = zip(pars, delta)\n",
    "\t\t          .map!(a => a[0] - a[1])\n",
    "\t\t          .array();\n",
    "\t\tparsDiff = zip(pars, parsPrev)\n",
    "\t\t                .map!(a => pow((a[0] - a[1]), 2.))\n",
    "\t\t                .reduce!((a, b) => a + b)\n",
    "\t\t                .sqrt;\n",
    "\t\tll = objFun(pars, x, y);\n",
    "\t\tllDiff = abs(ll - llPrev);\n",
    "\t\tparsPrev = pars;\n",
    "\t\tllPrev = ll;\n",
    "\t}\n",
    "\treturn pars;\n",
    "}\n",
    "```\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D code for Stochastic Gradient Descent\n",
    "\n",
    "The D implementation of the gradient function `nGradient` for the Normal log-likelihood is given below:\n",
    "\n",
    "<b>```\n",
    "/* For normal distribution gradient descent */\n",
    "T[] nGradient(T)(T[] pars, T[] x, T y)\n",
    "{\n",
    "\tT xSum = y - zip(pars, x).map!(a => a[0]*a[1])\n",
    "\t                   .reduce!((a, b) => a + b);\n",
    "\tT[] output = x.map!(a => a*xSum).array();\n",
    "\treturn output;\n",
    "}\n",
    "```\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The D implementation of the gradient descent function `gradientDescent` is given below:\n",
    "\n",
    "<b>\n",
    "```\n",
    "T[] gradientDescent(T, alias gradFun = nGradient)\n",
    "                     (T[] pars, T[][] x, T[] y, T alpha = 0.01,\n",
    "                     \tint nepochs = 500, T ptol = 1e-5)\n",
    "{\n",
    "\tauto prevPars = pars.dup;\n",
    "\tauto temp = pars.dup;\n",
    "\tfor(int i = 0; i < nepochs; ++i)\n",
    "\t{\n",
    "\t\tfor(int j = 0; j < x.length; ++j)\n",
    "\t\t{\n",
    "\t\t\tprevPars = pars.dup;\n",
    "\t\t\ttemp = gradFun(pars, x[j], y[j]);\n",
    "\t\t\tpars = zip(pars, temp)\n",
    "\t\t\t          .map!(a => a[0] + alpha*a[1])\n",
    "\t\t\t          .array();\n",
    "\t\t\ttemp = zip(pars, prevPars)\n",
    "\t\t\t          .map!(a => a[0] - a[1])\n",
    "\t\t\t          .array();\n",
    "\t\t\tauto norm = norm2(temp);\n",
    "\t\t\tassert(norm != T.infinity, \"infinite parameter issue\");\n",
    "\t\t\tif(norm < ptol)\n",
    "\t\t\t\tbreak;\n",
    "\t\t}\n",
    "\t}\n",
    "\treturn pars;\n",
    "}\n",
    "\n",
    "```\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of the `newtonRaphson` function is given below:\n",
    "\n",
    "<b>\n",
    "```\n",
    "auto z = read_yX(\"freeny.csv\");\n",
    "double[][] x;\n",
    "double[] y;\n",
    "y = z[0]; x = z[1];\n",
    "writeln(\"Output: \", newtonRaphson([1.,1.,1.,1.,1.], x, y));\n",
    "```\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is given by:<br>\n",
    "<b>`Output: [2.20867, 0.0125611, -0.0349353, 0.0378322, 0.0226276]`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of the `gradientDescent` function is given below:\n",
    "\n",
    "<b>```\n",
    "z = read_yX(\"ex3.csv\");\n",
    "\ty = z[0]; x = z[1];\n",
    "\twriteln(\"Output: \", gradientDescent([0.,0.,0.], x, y, 0.01, 500));\n",
    "```</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is given by:<br><b>`Output: [200561, 503985, -30934]`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R scripts showing that these outputs are correct are given in the [Gradient Descent script](https://github.com/dataPulverizer/glm-stochastic-gradient-descent-d/blob/master/code/gradientDescent.r) and the [Newton-Raphson script](https://github.com/dataPulverizer/glm-stochastic-gradient-descent-d/blob/master/code/newtonRaphson.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
