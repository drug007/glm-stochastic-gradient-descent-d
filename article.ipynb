{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Models and Stochastic Gradient Descent in D\n",
    "\n",
    "The [Mir GLAS](https://github.com/libmir/mir-glas) library has shown that D language is capable of high performance calculations to rival those written in C and C++. Mathematical analysis libraries native to D however are still in short supply. In this blog we roll our own gamma GLM model written in optimization style using the Newton-Raphson method and we carry out linear regression using gradient descent.\n",
    "\n",
    "# Mathematical Preliminaries\n",
    "\n",
    "The main aim of a regression algorithm is to find a set of coefficients ($\\beta$) that maximize the likelihood of a target variable ($y$) given a set of explanatory variables ($x$). The algorithm makes assumptions regarding the distribution of the target variable, and the independence of observations of the explanatory variables.\n",
    "\n",
    "The likelihood function represents assumption of the distribution of the target variable. The likelhood function for the Gamma distribution is given by:\n",
    "\n",
    "$$\n",
    "L(x) = \\frac{1}{\\Gamma(x)}\\theta^{k-1}_{-\\frac{x}{\\theta}}\n",
    "$$\n",
    "\n",
    "and the Normal distribution likelihood function is given by:\n",
    "\n",
    "$$\n",
    "L(x) = \\frac{1}{2\\pi \\sigma^2}\\exp^{-\\frac{(y - \\mu)^2}{2\\sigma}}\n",
    "$$\n",
    "\n",
    "In practice however, we actually maximise the log-likelihood ($l$) - that is the log of the likelihood function taken over the whole data set. The gradient function for the Gamma log-likelihood is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial \\beta_j} = -k x_j + k y x_j \\exp^{x\\beta}\n",
    "$$\n",
    "\n",
    "and its curvature is given by\n",
    "\n",
    "$$\n",
    "-y x_j x_l \\exp^{-x\\beta}\n",
    "$$\n",
    "\n",
    "# Regression as Optimization\n",
    "\n",
    "Regression is thus an optimization algorithm that maximizes the log-likelihood function for a set of $\\beta$ coefficients. It can thus be solved by numerical optimization.\n",
    "\n",
    "## Newton-Raphson Algorithm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
